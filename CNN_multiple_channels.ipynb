{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc07649",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b34919f",
   "metadata": {},
   "source": [
    "## Fully Connected Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8802f32",
   "metadata": {},
   "source": [
    "### Activation and Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "409d6b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "  def f(self, x):\n",
    "    return np.maximum(0, x)\n",
    "  \n",
    "  def fp(self, x):\n",
    "    return (x > 0).astype(float)\n",
    "  \n",
    "# Empty activation\n",
    "class EmptyActivation:\n",
    "  def f(self, x):\n",
    "    return x\n",
    "\n",
    "  def fp(self, x):\n",
    "    return np.ones_like(x)\n",
    "\n",
    "class SoftmaxCrossEntropy:\n",
    "    def f(self, y_true, logits):\n",
    "        # logits: shape (batch_size, num_classes)\n",
    "        # y_true: shape (batch_size,) with class indices\n",
    "\n",
    "        # Compute softmax\n",
    "        exps = np.exp(logits - np.max(logits, axis=1, keepdims=True))\n",
    "        probs = exps / np.sum(exps, axis=1, keepdims=True)\n",
    "        self.probs = probs\n",
    "\n",
    "        # Cross-entropy loss\n",
    "        batch_size = y_true.shape[0]\n",
    "        correct_logprobs = -np.log(probs[range(batch_size), y_true])\n",
    "        return np.mean(correct_logprobs)\n",
    "\n",
    "    def fp(self, y_true, logits):\n",
    "        # Derivative: probs - y_true_onehot\n",
    "        batch_size = y_true.shape[0]\n",
    "        grad = self.probs.copy()\n",
    "        grad[range(batch_size), y_true] -= 1\n",
    "        return grad / batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a69ad7",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66390f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified to be used with CNN\n",
    "class NeuralNetworkForCNN:\n",
    "  def __init__(self, input_shape: int, hidden_units: list[int], output_shape: int):\n",
    "    self.l = len(hidden_units) + 1  # number of layers\n",
    "    self.W = []                     # weights\n",
    "    self.b = []                     # biases\n",
    "    self.a = []                     # activations\n",
    "    \n",
    "    prev_units = input_shape\n",
    "    for units in hidden_units:\n",
    "      weight = np.random.randn(prev_units, units) * np.sqrt(2 / prev_units)\n",
    "      bias = np.zeros((1, units))\n",
    "      self.W.append(weight)\n",
    "      self.b.append(bias)\n",
    "      self.a.append(ReLU())\n",
    "      prev_units = units\n",
    "\n",
    "    # Output layer\n",
    "    weight = np.random.randn(prev_units, output_shape) * np.sqrt(2 / prev_units)\n",
    "    bias = np.zeros((1, output_shape))\n",
    "    self.W.append(weight)\n",
    "    self.b.append(bias)\n",
    "    self.a.append(EmptyActivation())\n",
    "\n",
    "    self.loss_fn = SoftmaxCrossEntropy()\n",
    "  \n",
    "  def set_activation(self, a: list):\n",
    "    self.a = a\n",
    "\n",
    "  def set_loss_fn(self, L):\n",
    "    self.loss_fn = L\n",
    "  \n",
    "  def forward(self, X):\n",
    "    x = X.copy()\n",
    "    zs = []  # pre-activation\n",
    "    As = []  # post-activation\n",
    "    for i in range(self.l):\n",
    "      z = x @ self.W[i] + self.b[i]\n",
    "      zs.append(z)\n",
    "      A = self.a[i].f(z)\n",
    "      As.append(A)\n",
    "      x = A\n",
    "    return zs, As\n",
    "  \n",
    "  def backward(self, X, y, zs, As, lr=0.01):\n",
    "    # Partial derivative of loss w.r.t. output\n",
    "    dL_dO = self.loss_fn.fp(y, As[-1])\n",
    "\n",
    "    # Gradient of layer i+1 w.r.t. layer i\n",
    "    dA = dL_dO\n",
    "    for i in range(self.l-1, -1, -1):\n",
    "      # Partial derivative of loss w.r.t. activation\n",
    "      a_deriv = self.a[i].fp(zs[i])\n",
    "      dL_dz = dA * a_deriv\n",
    "\n",
    "      # Update gradient for layer i-1\n",
    "      dA = dL_dz @ self.W[i].T\n",
    "\n",
    "      # Partial derivative of loss w.r.t. bias\n",
    "      dL_db = np.sum(dL_dz, axis=0, keepdims=True)\n",
    "\n",
    "      # Partial derivative of loss w.r.t. weight\n",
    "      dz_dW = As[i-1] if i > 0 else X\n",
    "      dL_dW = dz_dW.T @ dL_dz\n",
    "\n",
    "      # Update weights and biases\n",
    "      self.b[i] -= lr * dL_db\n",
    "      self.W[i] -= lr * dL_dW\n",
    "    return dA\n",
    "  \n",
    "  def get_batches(self, X, y, batch_size):\n",
    "    for i in range(0, len(X), batch_size):\n",
    "      yield X[i:i+batch_size], y[i:i+batch_size]\n",
    "\n",
    "\n",
    "  def train(self, X, y, epochs=100, lr=0.01, batch_size=32):\n",
    "    for epoch in range(epochs):\n",
    "      total_loss = 0\n",
    "      batches = self.get_batches(X, y, batch_size)\n",
    "\n",
    "      for X_batch, y_batch in batches:\n",
    "        zs, As = self.forward(X_batch)\n",
    "        loss = self.loss_fn.f(y_batch, As[-1])\n",
    "        total_loss += loss\n",
    "        self.backward(X_batch, y_batch, zs, As, lr)\n",
    "\n",
    "      avg_loss = total_loss / (len(X) // batch_size)\n",
    "\n",
    "  def test(self, X, y):\n",
    "    test_activations = self.forward(X)[-1]\n",
    "    y_pred = np.argmax(test_activations[-1], axis=1)\n",
    "    accuracy = np.mean(y_pred == y)\n",
    "    print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "  def train_and_test(self, X_train, y_train, X_test, y_test, epochs=100, lr=0.01, batch_size=32):\n",
    "    for epoch in range(epochs):\n",
    "      total_loss = 0\n",
    "      batches = self.get_batches(X_train, y_train, batch_size)\n",
    "\n",
    "      for X_batch, y_batch in batches:\n",
    "        zs, As = self.forward(X_batch)\n",
    "        loss = self.loss_fn.f(y_batch, As[-1])\n",
    "        total_loss += loss\n",
    "        self.backward(X_batch, y_batch, zs, As, lr)\n",
    "\n",
    "      avg_loss = total_loss / (len(X_train) // batch_size)\n",
    "      \n",
    "      test_activations = self.forward(X_test)[-1]\n",
    "      y_pred = np.argmax(test_activations[-1], axis=1)\n",
    "      accuracy = np.mean(y_pred == y_test)\n",
    "      \n",
    "      print(f\"Epoch {epoch}, Loss: {avg_loss:.4f}, Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e696977",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "040e9c79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 784),\n",
       " (10000, 784),\n",
       " (60000,),\n",
       " (10000,),\n",
       " ['T-shirt/top',\n",
       "  'Trouser',\n",
       "  'Pullover',\n",
       "  'Dress',\n",
       "  'Coat',\n",
       "  'Sandal',\n",
       "  'Shirt',\n",
       "  'Sneaker',\n",
       "  'Bag',\n",
       "  'Ankle boot'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load FashionMNIST dataset from pytorch\n",
    "train_dataset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=ToTensor())\n",
    "test_dataset = datasets.FashionMNIST(root=\"./data\", train=False, download=True, transform=ToTensor())\n",
    "class_names = train_dataset.classes\n",
    "\n",
    "# Extract data and labels\n",
    "X_train = train_dataset.data.numpy().reshape(-1, 28*28).astype(np.float32) / 255.0 # Normalize to [0, 1]\n",
    "y_train = train_dataset.targets.numpy()\n",
    "X_test = test_dataset.data.numpy().reshape(-1, 28*28).astype(np.float32) / 255.0 # Normalize to [0, 1]\n",
    "y_test = test_dataset.targets.numpy()\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape, class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36267c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.6924, Test Accuracy: 80.89%\n",
      "Epoch 1, Loss: 0.4858, Test Accuracy: 82.54%\n",
      "Epoch 2, Loss: 0.4426, Test Accuracy: 83.82%\n",
      "Epoch 3, Loss: 0.4160, Test Accuracy: 84.43%\n",
      "Epoch 4, Loss: 0.3965, Test Accuracy: 85.07%\n",
      "Epoch 5, Loss: 0.3809, Test Accuracy: 85.45%\n",
      "Epoch 6, Loss: 0.3679, Test Accuracy: 85.67%\n",
      "Epoch 7, Loss: 0.3565, Test Accuracy: 86.16%\n",
      "Epoch 8, Loss: 0.3466, Test Accuracy: 86.25%\n",
      "Epoch 9, Loss: 0.3375, Test Accuracy: 86.39%\n",
      "Epoch 10, Loss: 0.3291, Test Accuracy: 86.62%\n",
      "Epoch 11, Loss: 0.3215, Test Accuracy: 86.66%\n",
      "Epoch 12, Loss: 0.3142, Test Accuracy: 86.84%\n",
      "Epoch 13, Loss: 0.3076, Test Accuracy: 86.95%\n",
      "Epoch 14, Loss: 0.3014, Test Accuracy: 87.23%\n",
      "Epoch 15, Loss: 0.2957, Test Accuracy: 87.25%\n",
      "Epoch 16, Loss: 0.2902, Test Accuracy: 87.34%\n",
      "Epoch 17, Loss: 0.2851, Test Accuracy: 87.48%\n",
      "Epoch 18, Loss: 0.2802, Test Accuracy: 87.59%\n",
      "Epoch 19, Loss: 0.2754, Test Accuracy: 87.53%\n",
      "Epoch 20, Loss: 0.2710, Test Accuracy: 87.68%\n",
      "Epoch 21, Loss: 0.2668, Test Accuracy: 87.71%\n",
      "Epoch 22, Loss: 0.2625, Test Accuracy: 87.76%\n",
      "Epoch 23, Loss: 0.2583, Test Accuracy: 87.74%\n",
      "Epoch 24, Loss: 0.2543, Test Accuracy: 87.82%\n",
      "Epoch 25, Loss: 0.2507, Test Accuracy: 87.71%\n",
      "Epoch 26, Loss: 0.2469, Test Accuracy: 87.74%\n",
      "Epoch 27, Loss: 0.2434, Test Accuracy: 87.83%\n",
      "Epoch 28, Loss: 0.2401, Test Accuracy: 87.91%\n",
      "Epoch 29, Loss: 0.2364, Test Accuracy: 87.92%\n",
      "Epoch 30, Loss: 0.2332, Test Accuracy: 87.97%\n",
      "Epoch 31, Loss: 0.2299, Test Accuracy: 88.00%\n",
      "Epoch 32, Loss: 0.2268, Test Accuracy: 88.03%\n",
      "Epoch 33, Loss: 0.2237, Test Accuracy: 88.01%\n",
      "Epoch 34, Loss: 0.2205, Test Accuracy: 87.98%\n",
      "Epoch 35, Loss: 0.2174, Test Accuracy: 88.09%\n",
      "Epoch 36, Loss: 0.2145, Test Accuracy: 88.12%\n",
      "Epoch 37, Loss: 0.2115, Test Accuracy: 88.14%\n",
      "Epoch 38, Loss: 0.2088, Test Accuracy: 88.21%\n",
      "Epoch 39, Loss: 0.2060, Test Accuracy: 88.21%\n",
      "Epoch 40, Loss: 0.2034, Test Accuracy: 88.17%\n",
      "Epoch 41, Loss: 0.2006, Test Accuracy: 88.28%\n",
      "Epoch 42, Loss: 0.1980, Test Accuracy: 88.17%\n",
      "Epoch 43, Loss: 0.1955, Test Accuracy: 88.21%\n",
      "Epoch 44, Loss: 0.1929, Test Accuracy: 88.14%\n",
      "Epoch 45, Loss: 0.1903, Test Accuracy: 88.27%\n",
      "Epoch 46, Loss: 0.1878, Test Accuracy: 88.28%\n",
      "Epoch 47, Loss: 0.1855, Test Accuracy: 88.14%\n",
      "Epoch 48, Loss: 0.1830, Test Accuracy: 88.38%\n",
      "Epoch 49, Loss: 0.1804, Test Accuracy: 88.21%\n",
      "Epoch 50, Loss: 0.1781, Test Accuracy: 88.29%\n",
      "Epoch 51, Loss: 0.1758, Test Accuracy: 88.33%\n",
      "Epoch 52, Loss: 0.1733, Test Accuracy: 88.27%\n",
      "Epoch 53, Loss: 0.1715, Test Accuracy: 88.28%\n",
      "Epoch 54, Loss: 0.1690, Test Accuracy: 88.33%\n",
      "Epoch 55, Loss: 0.1668, Test Accuracy: 88.29%\n",
      "Epoch 56, Loss: 0.1648, Test Accuracy: 88.40%\n",
      "Epoch 57, Loss: 0.1625, Test Accuracy: 88.33%\n",
      "Epoch 58, Loss: 0.1603, Test Accuracy: 88.29%\n",
      "Epoch 59, Loss: 0.1583, Test Accuracy: 88.36%\n",
      "Epoch 60, Loss: 0.1562, Test Accuracy: 88.33%\n",
      "Epoch 61, Loss: 0.1544, Test Accuracy: 88.37%\n",
      "Epoch 62, Loss: 0.1522, Test Accuracy: 88.42%\n",
      "Epoch 63, Loss: 0.1504, Test Accuracy: 88.37%\n",
      "Epoch 64, Loss: 0.1482, Test Accuracy: 88.39%\n",
      "Epoch 65, Loss: 0.1462, Test Accuracy: 88.40%\n",
      "Epoch 66, Loss: 0.1445, Test Accuracy: 88.27%\n",
      "Epoch 67, Loss: 0.1423, Test Accuracy: 88.20%\n",
      "Epoch 68, Loss: 0.1405, Test Accuracy: 88.22%\n",
      "Epoch 69, Loss: 0.1385, Test Accuracy: 88.28%\n",
      "Epoch 70, Loss: 0.1367, Test Accuracy: 88.13%\n",
      "Epoch 71, Loss: 0.1352, Test Accuracy: 88.22%\n",
      "Epoch 72, Loss: 0.1332, Test Accuracy: 88.25%\n",
      "Epoch 73, Loss: 0.1316, Test Accuracy: 88.28%\n",
      "Epoch 74, Loss: 0.1301, Test Accuracy: 88.26%\n",
      "Epoch 75, Loss: 0.1280, Test Accuracy: 87.82%\n",
      "Epoch 76, Loss: 0.1266, Test Accuracy: 88.00%\n",
      "Epoch 77, Loss: 0.1248, Test Accuracy: 87.95%\n",
      "Epoch 78, Loss: 0.1233, Test Accuracy: 88.03%\n",
      "Epoch 79, Loss: 0.1218, Test Accuracy: 88.10%\n",
      "Epoch 80, Loss: 0.1201, Test Accuracy: 88.05%\n",
      "Epoch 81, Loss: 0.1184, Test Accuracy: 88.14%\n",
      "Epoch 82, Loss: 0.1169, Test Accuracy: 88.14%\n",
      "Epoch 83, Loss: 0.1153, Test Accuracy: 88.09%\n",
      "Epoch 84, Loss: 0.1138, Test Accuracy: 88.03%\n",
      "Epoch 85, Loss: 0.1120, Test Accuracy: 88.06%\n",
      "Epoch 86, Loss: 0.1108, Test Accuracy: 88.07%\n",
      "Epoch 87, Loss: 0.1090, Test Accuracy: 88.25%\n",
      "Epoch 88, Loss: 0.1078, Test Accuracy: 88.13%\n",
      "Epoch 89, Loss: 0.1065, Test Accuracy: 87.98%\n",
      "Epoch 90, Loss: 0.1049, Test Accuracy: 88.28%\n",
      "Epoch 91, Loss: 0.1035, Test Accuracy: 88.20%\n",
      "Epoch 92, Loss: 0.1023, Test Accuracy: 88.07%\n",
      "Epoch 93, Loss: 0.1005, Test Accuracy: 88.15%\n",
      "Epoch 94, Loss: 0.0994, Test Accuracy: 88.25%\n",
      "Epoch 95, Loss: 0.0980, Test Accuracy: 88.02%\n",
      "Epoch 96, Loss: 0.0967, Test Accuracy: 88.16%\n",
      "Epoch 97, Loss: 0.0955, Test Accuracy: 88.30%\n",
      "Epoch 98, Loss: 0.0939, Test Accuracy: 88.08%\n",
      "Epoch 99, Loss: 0.0928, Test Accuracy: 88.40%\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Train and test the model\n",
    "model = NeuralNetworkForCNN(input_shape=784, hidden_units=[128, 64], output_shape=10)\n",
    "model.train_and_test(X_train, y_train, X_test, y_test, epochs=100, lr=0.01, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea9265b",
   "metadata": {},
   "source": [
    "## CNN Without Padding and Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b4100cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_NoPaddingNoPooling:\n",
    "  def __init__(self, input_shape, nn_hidden_units, output_shape, kernel_size, stride=1):\n",
    "    self.channels, self.n, _ = input_shape # (channels, height, width) = (C, H, W), assume square image\n",
    "    self.stride = stride # Use stride=1 for now\n",
    "    cnn_shape = ((self.n - kernel_size) // self.stride) + 1 # Final shape after convolution\n",
    "    \n",
    "    self.kernel = np.random.randn(self.channels, kernel_size, kernel_size) * np.sqrt(2 / (self.channels * kernel_size * kernel_size))\n",
    "    self.bias = np.zeros((1,))\n",
    "    self.activation = ReLU()\n",
    "    self.nn = NeuralNetworkForCNN(input_shape=cnn_shape**2, hidden_units=nn_hidden_units, output_shape=output_shape)\n",
    "\n",
    "  # Only 2D convolution is implemented\n",
    "  def conv2d(self, X_batch, kernel):\n",
    "    batch_size, channels, n, _ = X_batch.shape\n",
    "    k = kernel.shape[-1]\n",
    "    final_shape = ((n - k) // self.stride) + 1\n",
    "    output = np.zeros((batch_size, final_shape, final_shape))\n",
    "\n",
    "    for i in range(0, n - k + 1, self.stride):\n",
    "      for j in range(0, n - k + 1, self.stride):\n",
    "        region = X_batch[:, :, i:i+k, j:j+k]                       # shape: (batch_size, channels, k, k)\n",
    "        output[:, i, j] = np.sum(region * kernel, axis=(1, 2, 3))  # sum over (channels, k, k)\n",
    "\n",
    "    return output\n",
    "\n",
    "  def forward(self, X_batch):\n",
    "    Z = self.conv2d(X_batch, self.kernel) + self.bias\n",
    "    A = self.activation.f(Z)\n",
    "    A_flat = A.reshape(A.shape[0], -1)  # flatten each sample\n",
    "    nn_zs, nn_As = self.nn.forward(A_flat)\n",
    "    return Z, A, A_flat, nn_zs, nn_As\n",
    "  \n",
    "  def backward(self, X_batch, y_batch, z, A, A_flat, nn_zs, nn_As, lr=0.1):\n",
    "    dL_dA_flat = self.nn.backward(A_flat, y_batch, nn_zs, nn_As, lr)\n",
    "    dL_dA = dL_dA_flat.reshape(A.shape)           # Reshape back to original shape\n",
    "\n",
    "    dA_dz = self.activation.fp(z)                 # Gradient of activation function\n",
    "    dL_dz = dL_dA * dA_dz                         # Gradient of loss w.r.t. z\n",
    "\n",
    "    # Gradient of loss w.r.t. bias\n",
    "    dL_db = np.sum(dL_dz, axis=(0, 1, 2))\n",
    "\n",
    "    # Convolution backward: convolve input with dL_dz\n",
    "    dL_dk = np.zeros_like(self.kernel)\n",
    "    for b in range(X_batch.shape[0]):\n",
    "      grad = self.conv2d(X_batch[b:b+1], dL_dz[b:b+1])\n",
    "      dL_dk += grad\n",
    "\n",
    "    # Average gradients over batch size\n",
    "    dL_dk /= X_batch.shape[0]\n",
    "\n",
    "    # Update kernel and bias\n",
    "    self.kernel -= lr * dL_dk\n",
    "    self.bias -= lr * dL_db\n",
    "\n",
    "  def get_batches(self, X, y, batch_size):\n",
    "    for i in range(0, len(X), batch_size):\n",
    "      yield X[i:i+batch_size], y[i:i+batch_size]\n",
    "\n",
    "  def train_and_test(self, X_train, y_train, X_test, y_test, epochs=100, lr=0.1, batch_size=32):\n",
    "    for epoch in range(epochs):\n",
    "      total_loss = 0\n",
    "      batches = self.nn.get_batches(X_train, y_train, batch_size)\n",
    "\n",
    "      for X_batch, y_batch in batches:\n",
    "        Z, A, A_flat, nn_zs, nn_As = self.forward(X_batch)\n",
    "        loss = self.nn.loss_fn.f(y_batch, nn_As[-1])\n",
    "        total_loss += loss\n",
    "        self.backward(X_batch, y_batch, Z, A, A_flat, nn_zs, nn_As, lr)\n",
    "\n",
    "      avg_loss = total_loss / (len(X_train) // batch_size)\n",
    "\n",
    "      # Test after each epoch\n",
    "      test_activations = self.forward(X_test)[-1]\n",
    "      y_pred = np.argmax(test_activations[-1], axis=1)\n",
    "      accuracy = np.mean(y_pred == y_test)\n",
    "      \n",
    "      print(f\"Epoch {epoch}, Loss: {avg_loss:.4f}, Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6fad6fb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 1, 28, 28),\n",
       " (10000, 1, 28, 28),\n",
       " (60000,),\n",
       " (10000,),\n",
       " ['T-shirt/top',\n",
       "  'Trouser',\n",
       "  'Pullover',\n",
       "  'Dress',\n",
       "  'Coat',\n",
       "  'Sandal',\n",
       "  'Shirt',\n",
       "  'Sneaker',\n",
       "  'Bag',\n",
       "  'Ankle boot'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load FashionMNIST dataset from pytorch\n",
    "train_dataset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=ToTensor())\n",
    "test_dataset = datasets.FashionMNIST(root=\"./data\", train=False, download=True, transform=ToTensor())\n",
    "class_names = train_dataset.classes\n",
    "\n",
    "# Extract data and labels\n",
    "X_train = train_dataset.data.numpy().astype(np.float32) / 255.0 # Normalize to [0, 1]\n",
    "y_train = train_dataset.targets.numpy()\n",
    "X_test = test_dataset.data.numpy().astype(np.float32) / 255.0 # Normalize to [0, 1]\n",
    "y_test = test_dataset.targets.numpy()\n",
    "\n",
    "X_train = X_train.reshape(-1, 1, 28, 28)  # Reshape to (batch_size, channels, height, width)\n",
    "X_test = X_test.reshape(-1, 1, 28, 28)    # Reshape to (batch_size, channels, height, width)\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape, class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f34089c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.5789, Test Accuracy: 82.06%\n",
      "Epoch 1, Loss: 0.4184, Test Accuracy: 83.89%\n",
      "Epoch 2, Loss: 0.3730, Test Accuracy: 84.72%\n",
      "Epoch 3, Loss: 0.3460, Test Accuracy: 85.85%\n",
      "Epoch 4, Loss: 0.3248, Test Accuracy: 86.60%\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Train and test the model\n",
    "model = CNN_NoPaddingNoPooling(input_shape=X_train.shape[1:], nn_hidden_units=[128, 64], output_shape=10, kernel_size=3, stride=1)\n",
    "model.train_and_test(X_train, y_train, X_test, y_test, epochs=5, lr=0.1, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec663e5",
   "metadata": {},
   "source": [
    "## CNN No Padding, With Max Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ccd254f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_NoPadding_WithPooling:\n",
    "  def __init__(self, input_shape, nn_hidden_units, output_shape, kernel_size, stride=1, pool_size=2, pool_stride=2):\n",
    "    self.channels, self.n, _ = input_shape # (channels, height, width) = (C, H, W), assume square image\n",
    "\n",
    "    # Convolution layer\n",
    "    self.kernel_size = kernel_size\n",
    "    self.stride = stride\n",
    "    self.cnn_shape = ((self.n - kernel_size) // self.stride) + 1 # Final shape after convolution\n",
    "    \n",
    "    self.kernel = np.random.randn(self.channels, kernel_size, kernel_size) * np.sqrt(2 / (self.channels * kernel_size * kernel_size))\n",
    "    self.bias = np.zeros((1,))\n",
    "    self.activation = ReLU()\n",
    "\n",
    "    # Pooling layer\n",
    "    self.pool_size = pool_size\n",
    "    self.pool_stride = pool_stride\n",
    "\n",
    "    # Fully connected layer\n",
    "    self.pooled_shape = ((self.cnn_shape - pool_size) // pool_stride) + 1   # Final shape after pooling\n",
    "    self.nn = NeuralNetworkForCNN(input_shape=self.pooled_shape**2, hidden_units=nn_hidden_units, output_shape=output_shape)\n",
    "\n",
    "  # Only 2D convolution is implemented\n",
    "  def conv2d(self, X_batch, kernel):\n",
    "    batch_size, channels, n, _ = X_batch.shape\n",
    "    k = kernel.shape[-1]\n",
    "    final_shape = ((n - k) // self.stride) + 1\n",
    "    output = np.zeros((batch_size, final_shape, final_shape))\n",
    "\n",
    "    for i in range(0, n - k + 1, self.stride):\n",
    "      for j in range(0, n - k + 1, self.stride):\n",
    "        region = X_batch[:, :, i:i+k, j:j+k]                       # shape: (batch_size, channels, k, k)\n",
    "        output[:, i, j] = np.sum(region * kernel, axis=(1, 2, 3))  # sum over (channels, k, k)\n",
    "\n",
    "    return output\n",
    "\n",
    "  # Only 2D pooling is implemented, use max pooling for now\n",
    "  def max_pool2d(self, X):\n",
    "    batch_size, n, _ = X.shape\n",
    "    final_shape = ((n - self.pool_size) // self.pool_stride) + 1\n",
    "    output = np.zeros((batch_size, final_shape, final_shape))\n",
    "\n",
    "    for i in range(0, n - self.pool_size + 1, self.pool_stride):\n",
    "      for j in range(0, n - self.pool_size + 1, self.pool_stride):\n",
    "        region = X[:, i:i+self.pool_size, j:j+self.pool_size]  # (batch_size, pool_size, pool_size)\n",
    "        output[:, i // self.pool_stride, j // self.pool_stride] = np.max(region, axis=(1, 2))\n",
    "\n",
    "    return output\n",
    "\n",
    "  def forward(self, X_batch):\n",
    "    Z = self.conv2d(X_batch, self.kernel) + self.bias     # Convolution\n",
    "    A = self.activation.f(Z)                              # Activation\n",
    "    A_pool = self.max_pool2d(A)                           # Pooling\n",
    "    A_flat = A_pool.reshape(A_pool.shape[0], -1)          # Flatten\n",
    "    nn_zs, nn_As = self.nn.forward(A_flat)                # Fully connected\n",
    "    return Z, A, A_pool, A_flat, nn_zs, nn_As\n",
    "  \n",
    "  # Compute backward propapagation for max pooling\n",
    "  def max_pool2d_backward(self, dL_dA_pool, A):\n",
    "    batch_size, n, _ = A.shape\n",
    "    dL_dA = np.zeros_like(A)\n",
    "\n",
    "    for i in range(0, n - self.pool_size + 1, self.pool_stride):\n",
    "      for j in range(0, n - self.pool_size + 1, self.pool_stride):\n",
    "        region = A[:, i:i+self.pool_size, j:j+self.pool_size]             # (batch_size, pool_size, pool_size)\n",
    "        max_mask = np.max(region, axis=(1, 2), keepdims=True) == region   # Mask for max values\n",
    "        dL_dA[:, i:i+self.pool_size, j:j+self.pool_size] += max_mask * dL_dA_pool[:, i // self.pool_stride, j // self.pool_stride][:, np.newaxis, np.newaxis]\n",
    "\n",
    "    return dL_dA\n",
    "  \n",
    "  def backward(self, X_batch, y_batch, z, A, A_pool, A_flat, nn_zs, nn_As, lr=0.1):\n",
    "    dL_dA_flat = self.nn.backward(A_flat, y_batch, nn_zs, nn_As, lr)\n",
    "    dL_dA_pool = dL_dA_flat.reshape(A_pool.shape)           # Reshape back to original shape\n",
    "    \n",
    "    dL_dA = self.max_pool2d_backward(dL_dA_pool, A)    # Backprop through pooling\n",
    "\n",
    "    dA_dz = self.activation.fp(z)                      # Gradient of activation function\n",
    "    dL_dz = dL_dA * dA_dz                              # Gradient of loss w.r.t. z\n",
    "\n",
    "    # Gradient of loss w.r.t. bias\n",
    "    dL_db = np.sum(dL_dz, axis=(0, 1, 2))\n",
    "\n",
    "    # Convolution backward: convolve input with dL_dz\n",
    "    dL_dk = np.zeros_like(self.kernel)\n",
    "    for b in range(X_batch.shape[0]):\n",
    "      grad = self.conv2d(X_batch[b:b+1], dL_dz[b:b+1])\n",
    "      dL_dk += grad\n",
    "\n",
    "    # Average gradients over batch size\n",
    "    dL_dk /= X_batch.shape[0]\n",
    "\n",
    "    # Update kernel and bias\n",
    "    self.kernel -= lr * dL_dk\n",
    "    self.bias -= lr * dL_db\n",
    "\n",
    "  def get_batches(self, X, y, batch_size):\n",
    "    for i in range(0, len(X), batch_size):\n",
    "      yield X[i:i+batch_size], y[i:i+batch_size]\n",
    "\n",
    "  def train_and_test(self, X_train, y_train, X_test, y_test, epochs=100, lr=0.1, batch_size=32):\n",
    "    for epoch in range(epochs):\n",
    "      total_loss = 0\n",
    "      batches = self.nn.get_batches(X_train, y_train, batch_size)\n",
    "\n",
    "      for X_batch, y_batch in batches:\n",
    "        Z, A, A_pool, A_flat, nn_zs, nn_As = self.forward(X_batch)\n",
    "        loss = self.nn.loss_fn.f(y_batch, nn_As[-1])\n",
    "        total_loss += loss\n",
    "        self.backward(X_batch, y_batch, Z, A, A_pool, A_flat, nn_zs, nn_As, lr)\n",
    "\n",
    "      avg_loss = total_loss / (len(X_train) // batch_size)\n",
    "\n",
    "      # Test after each epoch\n",
    "      test_activations = self.forward(X_test)[-1]\n",
    "      y_pred = np.argmax(test_activations[-1], axis=1)\n",
    "      accuracy = np.mean(y_pred == y_test)\n",
    "      \n",
    "      print(f\"Epoch {epoch}, Loss: {avg_loss:.4f}, Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e59c79a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 1, 28, 28),\n",
       " (10000, 1, 28, 28),\n",
       " (60000,),\n",
       " (10000,),\n",
       " ['T-shirt/top',\n",
       "  'Trouser',\n",
       "  'Pullover',\n",
       "  'Dress',\n",
       "  'Coat',\n",
       "  'Sandal',\n",
       "  'Shirt',\n",
       "  'Sneaker',\n",
       "  'Bag',\n",
       "  'Ankle boot'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load FashionMNIST dataset from pytorch\n",
    "train_dataset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=ToTensor())\n",
    "test_dataset = datasets.FashionMNIST(root=\"./data\", train=False, download=True, transform=ToTensor())\n",
    "class_names = train_dataset.classes\n",
    "\n",
    "# Extract data and labels\n",
    "X_train = train_dataset.data.numpy().astype(np.float32) / 255.0 # Normalize to [0, 1]\n",
    "y_train = train_dataset.targets.numpy()\n",
    "X_test = test_dataset.data.numpy().astype(np.float32) / 255.0 # Normalize to [0, 1]\n",
    "y_test = test_dataset.targets.numpy()\n",
    "\n",
    "X_train = X_train.reshape(-1, 1, 28, 28)  # Reshape to (batch_size, channels, height, width)\n",
    "X_test = X_test.reshape(-1, 1, 28, 28)    # Reshape to (batch_size, channels, height, width)\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape, class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "626551a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.6298, Test Accuracy: 77.75%\n",
      "Epoch 1, Loss: 0.4820, Test Accuracy: 82.07%\n",
      "Epoch 2, Loss: 0.4399, Test Accuracy: 82.76%\n",
      "Epoch 3, Loss: 0.4128, Test Accuracy: 83.61%\n",
      "Epoch 4, Loss: 0.3942, Test Accuracy: 84.22%\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Train and test the model\n",
    "model = CNN_NoPadding_WithPooling(input_shape=X_train.shape[1:], \n",
    "                                  nn_hidden_units=[128, 64], \n",
    "                                  output_shape=10, \n",
    "                                  kernel_size=3, \n",
    "                                  stride=1, \n",
    "                                  pool_size=2, \n",
    "                                  pool_stride=2)\n",
    "model.train_and_test(X_train, y_train, X_test, y_test, epochs=5, lr=0.1, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65517d7",
   "metadata": {},
   "source": [
    "## CNN With Padding and Max Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a3893be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_WithPadding_WithPooling:\n",
    "  def __init__(self, input_shape, nn_hidden_units, output_shape, kernel_size, stride=1, padding_valid=True, pool_size=2, pool_stride=2):\n",
    "    self.channels, self.n, _ = input_shape # (channels, height, width) = (C, H, W), assume square image\n",
    "\n",
    "    # Convolution layer\n",
    "    self.kernel_size = kernel_size\n",
    "    self.stride = stride\n",
    "    self.padding_valid = padding_valid # true for valid padding, false for same padding\n",
    "\n",
    "    if padding_valid:\n",
    "      self.cnn_shape = ((self.n - kernel_size) // self.stride) + 1 # Final shape after convolution\n",
    "    else:\n",
    "      self.pad =  (kernel_size - 1) // 2\n",
    "      self.cnn_shape = ((self.n + 2*self.pad - kernel_size) // self.stride) + 1\n",
    "    \n",
    "    self.kernel = np.random.randn(self.channels, kernel_size, kernel_size) * np.sqrt(2 / (self.channels * kernel_size * kernel_size))\n",
    "    self.bias = np.zeros((1,))\n",
    "    self.activation = ReLU()\n",
    "\n",
    "    # Pooling layer\n",
    "    self.pool_size = pool_size\n",
    "    self.pool_stride = pool_stride\n",
    "\n",
    "    # Fully connected layer\n",
    "    self.pooled_shape = ((self.cnn_shape - pool_size) // pool_stride) + 1   # Final shape after pooling\n",
    "    self.nn = NeuralNetworkForCNN(input_shape=self.pooled_shape**2, hidden_units=nn_hidden_units, output_shape=output_shape)\n",
    "\n",
    "  # Only 2D convolution is implemented\n",
    "  def conv2d(self, X_batch, kernel, padding_valid):\n",
    "    batch_size, channels, n, _ = X_batch.shape\n",
    "    k = kernel.shape[-1]\n",
    "\n",
    "    if not padding_valid:\n",
    "      X_batch = np.pad(X_batch, ((0, 0), (0, 0), (self.pad, self.pad), (self.pad, self.pad)), mode='constant')\n",
    "      n += 2 * self.pad\n",
    "\n",
    "    final_shape = ((n - k) // self.stride) + 1\n",
    "    output = np.zeros((batch_size, final_shape, final_shape))\n",
    "\n",
    "    for i in range(0, n - k + 1, self.stride):\n",
    "      for j in range(0, n - k + 1, self.stride):\n",
    "        region = X_batch[:, :, i:i+k, j:j+k]                       # shape: (batch_size, channels, k, k)\n",
    "        output[:, i, j] = np.sum(region * kernel, axis=(1, 2, 3))  # sum over (channels, k, k)\n",
    "\n",
    "    return output\n",
    "\n",
    "  # Only 2D pooling is implemented, use max pooling for now\n",
    "  def max_pool2d(self, X):\n",
    "    batch_size, n, _ = X.shape\n",
    "    final_shape = ((n - self.pool_size) // self.pool_stride) + 1\n",
    "    output = np.zeros((batch_size, final_shape, final_shape))\n",
    "\n",
    "    for i in range(0, n - self.pool_size + 1, self.pool_stride):\n",
    "      for j in range(0, n - self.pool_size + 1, self.pool_stride):\n",
    "        region = X[:, i:i+self.pool_size, j:j+self.pool_size]  # (batch_size, pool_size, pool_size)\n",
    "        output[:, i // self.pool_stride, j // self.pool_stride] = np.max(region, axis=(1, 2))\n",
    "\n",
    "    return output\n",
    "\n",
    "  def forward(self, X_batch):\n",
    "    Z = self.conv2d(X_batch, self.kernel, self.padding_valid) + self.bias     # Convolution\n",
    "    A = self.activation.f(Z)                                                  # Activation\n",
    "    A_pool = self.max_pool2d(A)                                               # Pooling\n",
    "    A_flat = A_pool.reshape(A_pool.shape[0], -1)                              # Flatten\n",
    "    nn_zs, nn_As = self.nn.forward(A_flat)                                    # Fully connected\n",
    "    return Z, A, A_pool, A_flat, nn_zs, nn_As\n",
    "  \n",
    "  # Compute backward propapagation for max pooling\n",
    "  def max_pool2d_backward(self, dL_dA_pool, A):\n",
    "    batch_size, n, _ = A.shape\n",
    "    dL_dA = np.zeros_like(A)\n",
    "\n",
    "    for i in range(0, n - self.pool_size + 1, self.pool_stride):\n",
    "      for j in range(0, n - self.pool_size + 1, self.pool_stride):\n",
    "        region = A[:, i:i+self.pool_size, j:j+self.pool_size]             # (batch_size, pool_size, pool_size)\n",
    "        max_mask = np.max(region, axis=(1, 2), keepdims=True) == region   # Mask for max values\n",
    "        dL_dA[:, i:i+self.pool_size, j:j+self.pool_size] += max_mask * dL_dA_pool[:, i // self.pool_stride, j // self.pool_stride][:, np.newaxis, np.newaxis]\n",
    "\n",
    "    return dL_dA\n",
    "  \n",
    "  def backward(self, X_batch, y_batch, z, A, A_pool, A_flat, nn_zs, nn_As, lr=0.1):\n",
    "    dL_dA_flat = self.nn.backward(A_flat, y_batch, nn_zs, nn_As, lr)\n",
    "    dL_dA_pool = dL_dA_flat.reshape(A_pool.shape)           # Reshape back to original shape\n",
    "    \n",
    "    dL_dA = self.max_pool2d_backward(dL_dA_pool, A)    # Backprop through pooling\n",
    "\n",
    "    dA_dz = self.activation.fp(z)                      # Gradient of activation function\n",
    "    dL_dz = dL_dA * dA_dz                              # Gradient of loss w.r.t. z\n",
    "\n",
    "    # Gradient of loss w.r.t. bias\n",
    "    dL_db = np.sum(dL_dz)\n",
    "\n",
    "    # Convolution backward: convolve input with dL_dz\n",
    "    dL_dk = np.zeros_like(self.kernel)\n",
    "    for b in range(X_batch.shape[0]):\n",
    "      grad = self.conv2d(X_batch[b:b+1], dL_dz[b:b+1], padding_valid=self.padding_valid)\n",
    "      dL_dk += grad\n",
    "\n",
    "    # Average gradients over batch size\n",
    "    dL_dk /= X_batch.shape[0]\n",
    "\n",
    "    # Update kernel and bias\n",
    "    self.kernel -= lr * dL_dk\n",
    "    self.bias -= lr * dL_db\n",
    "\n",
    "  def get_batches(self, X, y, batch_size):\n",
    "    for i in range(0, len(X), batch_size):\n",
    "      yield X[i:i+batch_size], y[i:i+batch_size]\n",
    "\n",
    "  def train_and_test(self, X_train, y_train, X_test, y_test, epochs=100, lr=0.1, batch_size=32):\n",
    "    for epoch in range(epochs):\n",
    "      total_loss = 0\n",
    "      batches = self.nn.get_batches(X_train, y_train, batch_size)\n",
    "\n",
    "      for X_batch, y_batch in batches:\n",
    "        Z, A, A_pool, A_flat, nn_zs, nn_As = self.forward(X_batch)\n",
    "        loss = self.nn.loss_fn.f(y_batch, nn_As[-1])\n",
    "        total_loss += loss\n",
    "        self.backward(X_batch, y_batch, Z, A, A_pool, A_flat, nn_zs, nn_As, lr)\n",
    "\n",
    "      avg_loss = total_loss / (len(X_train) // batch_size)\n",
    "\n",
    "      # Test after each epoch\n",
    "      test_activations = self.forward(X_test)[-1]\n",
    "      y_pred = np.argmax(test_activations[-1], axis=1)\n",
    "      accuracy = np.mean(y_pred == y_test)\n",
    "      \n",
    "      print(f\"Epoch {epoch}, Loss: {avg_loss:.4f}, Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "10325c48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 1, 28, 28),\n",
       " (10000, 1, 28, 28),\n",
       " (60000,),\n",
       " (10000,),\n",
       " ['T-shirt/top',\n",
       "  'Trouser',\n",
       "  'Pullover',\n",
       "  'Dress',\n",
       "  'Coat',\n",
       "  'Sandal',\n",
       "  'Shirt',\n",
       "  'Sneaker',\n",
       "  'Bag',\n",
       "  'Ankle boot'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load FashionMNIST dataset from pytorch\n",
    "train_dataset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=ToTensor())\n",
    "test_dataset = datasets.FashionMNIST(root=\"./data\", train=False, download=True, transform=ToTensor())\n",
    "class_names = train_dataset.classes\n",
    "\n",
    "# Extract data and labels\n",
    "X_train = train_dataset.data.numpy().astype(np.float32) / 255.0 # Normalize to [0, 1]\n",
    "y_train = train_dataset.targets.numpy()\n",
    "X_test = test_dataset.data.numpy().astype(np.float32) / 255.0 # Normalize to [0, 1]\n",
    "y_test = test_dataset.targets.numpy()\n",
    "\n",
    "X_train = X_train.reshape(-1, 1, 28, 28)  # Reshape to (batch_size, channels, height, width)\n",
    "X_test = X_test.reshape(-1, 1, 28, 28)    # Reshape to (batch_size, channels, height, width)\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape, class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "93d31b64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.5804, Test Accuracy: 80.97%\n",
      "Epoch 1, Loss: 0.4320, Test Accuracy: 83.82%\n",
      "Epoch 2, Loss: 0.3895, Test Accuracy: 84.90%\n",
      "Epoch 3, Loss: 0.3628, Test Accuracy: 85.62%\n",
      "Epoch 4, Loss: 0.3427, Test Accuracy: 85.93%\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Train and test the model\n",
    "model = CNN_WithPadding_WithPooling(input_shape=X_train.shape[1:], \n",
    "                                  nn_hidden_units=[128, 64], \n",
    "                                  output_shape=10, \n",
    "                                  kernel_size=3, \n",
    "                                  stride=1, \n",
    "                                  padding_valid=False,\n",
    "                                  pool_size=2, \n",
    "                                  pool_stride=2)\n",
    "model.train_and_test(X_train, y_train, X_test, y_test, epochs=5, lr=0.1, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fffab88",
   "metadata": {},
   "source": [
    "## CNN With Multiple Filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b012fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Multiple_Filters:\n",
    "  def __init__(self, input_shape, nn_hidden_units, output_shape, kernel_size, stride=1, padding_valid=True, num_kernels=1, pool_size=2, pool_stride=2):\n",
    "    self.channels, self.n, _ = input_shape # (channels, height, width) = (C, H, W), assume square image\n",
    "\n",
    "    # Convolution layer\n",
    "    self.kernel_size = kernel_size\n",
    "    self.stride = stride\n",
    "    self.padding_valid = padding_valid # true for valid padding, false for same padding\n",
    "    self.num_kernels = num_kernels\n",
    "\n",
    "    if padding_valid:\n",
    "      self.cnn_shape = ((self.n - kernel_size) // self.stride) + 1 # Final shape after convolution\n",
    "    else:\n",
    "      self.pad =  (kernel_size - 1) // 2\n",
    "      self.cnn_shape = ((self.n + 2*self.pad - kernel_size) // self.stride) + 1\n",
    "    \n",
    "    self.kernel = np.random.randn(self.num_kernels, self.channels, kernel_size, kernel_size) * np.sqrt(2 / (self.channels * kernel_size * kernel_size))\n",
    "    self.bias = np.zeros((self.num_kernels,))\n",
    "    self.activation = ReLU()\n",
    "\n",
    "    # Pooling layer\n",
    "    self.pool_size = pool_size\n",
    "    self.pool_stride = pool_stride\n",
    "\n",
    "    # Fully connected layer\n",
    "    self.pooled_shape = ((self.cnn_shape - pool_size) // pool_stride) + 1   # Final shape after pooling\n",
    "    self.nn = NeuralNetworkForCNN(input_shape=num_kernels*(self.pooled_shape**2), hidden_units=nn_hidden_units, output_shape=output_shape)\n",
    "\n",
    "  # Only 2D convolution is implemented\n",
    "  def conv2d(self, X_batch, kernel, padding_valid):\n",
    "    batch_size, channels, n, _ = X_batch.shape\n",
    "    num_kernels, k = kernel.shape[0], kernel.shape[-1]\n",
    "\n",
    "    if not padding_valid:\n",
    "      X_batch = np.pad(X_batch, ((0, 0), (0, 0), (self.pad, self.pad), (self.pad, self.pad)), mode='constant')\n",
    "      n += 2 * self.pad\n",
    "\n",
    "    final_shape = ((n - k) // self.stride) + 1\n",
    "    output = np.zeros((batch_size, num_kernels, final_shape, final_shape))\n",
    "\n",
    "    for f in range(num_kernels):\n",
    "      for i in range(0, n - k + 1, self.stride):\n",
    "        for j in range(0, n - k + 1, self.stride):\n",
    "          region = X_batch[:, :, i:i+k, j:j+k]                       # shape: (batch_size, channels, k, k)\n",
    "          output[:, f, i, j] = np.sum(region * kernel[f], axis=(1, 2, 3))  # sum over (channels, k, k)\n",
    "\n",
    "    return output\n",
    "\n",
    "  # Only 2D pooling is implemented, use max pooling for now\n",
    "  def max_pool2d(self, X):\n",
    "    batch_size, num_kernels, n, _ = X.shape\n",
    "    final_shape = ((n - self.pool_size) // self.pool_stride) + 1\n",
    "    output = np.zeros((batch_size, num_kernels, final_shape, final_shape))\n",
    "\n",
    "    for i in range(0, n - self.pool_size + 1, self.pool_stride):\n",
    "      for j in range(0, n - self.pool_size + 1, self.pool_stride):\n",
    "        region = X[:, :, i:i+self.pool_size, j:j+self.pool_size]  # (batch_size, num_kernels, pool_size, pool_size)\n",
    "        output[:, :, i // self.pool_stride, j // self.pool_stride] = np.max(region, axis=(2, 3))\n",
    "\n",
    "    return output\n",
    "\n",
    "  def forward(self, X_batch):\n",
    "    Z = self.conv2d(X_batch, self.kernel, self.padding_valid) + self.bias[None, :, None, None]     # Convolution\n",
    "    A = self.activation.f(Z)                                                                       # Activation\n",
    "    A_pool = self.max_pool2d(A)                                                                    # Pooling\n",
    "    A_flat = A_pool.reshape(A_pool.shape[0], -1)                                                   # Flatten\n",
    "    nn_zs, nn_As = self.nn.forward(A_flat)                                                         # Fully connected\n",
    "    return Z, A, A_pool, A_flat, nn_zs, nn_As\n",
    "  \n",
    "  # Compute backward propapagation for max pooling\n",
    "  def max_pool2d_backward(self, dL_dA_pool, A):\n",
    "    batch_size, num_kernels, n, _ = A.shape\n",
    "    dL_dA = np.zeros_like(A)\n",
    "\n",
    "    for f in range(num_kernels):\n",
    "      for i in range(0, n - self.pool_size + 1, self.pool_stride):\n",
    "        for j in range(0, n - self.pool_size + 1, self.pool_stride):\n",
    "          region = A[:, f, i:i+self.pool_size, j:j+self.pool_size]             # (batch_size, num_kernels, pool_size, pool_size)\n",
    "          max_mask = np.max(region, axis=(1, 2), keepdims=True) == region      # Mask for max values\n",
    "          grad = dL_dA_pool[:, f, i // self.pool_stride, j // self.pool_stride][:, np.newaxis, np.newaxis]\n",
    "          dL_dA[:, f, i:i+self.pool_size, j:j+self.pool_size] += max_mask * grad\n",
    "\n",
    "    return dL_dA\n",
    "  \n",
    "  def backward(self, X_batch, y_batch, z, A, A_pool, A_flat, nn_zs, nn_As, lr=0.1):\n",
    "    dL_dA_flat = self.nn.backward(A_flat, y_batch, nn_zs, nn_As, lr)\n",
    "    dL_dA_pool = dL_dA_flat.reshape(A_pool.shape)           # Reshape back to original shape\n",
    "    \n",
    "    dL_dA = self.max_pool2d_backward(dL_dA_pool, A)    # Backprop through pooling\n",
    "\n",
    "    dA_dz = self.activation.fp(z)                      # Gradient of activation function\n",
    "    dL_dz = dL_dA * dA_dz                              # Gradient of loss w.r.t. z\n",
    "\n",
    "    # Gradient of loss w.r.t. bias\n",
    "    dL_db = np.sum(dL_dz, axis=(0, 2, 3))  # Sum over batch size and spatial dimensions\n",
    "\n",
    "    # Convolution backward: convolve input with dL_dz\n",
    "    dL_dk = np.zeros_like(self.kernel)\n",
    "    for b in range(X_batch.shape[0]):\n",
    "      input_ = X_batch[b:b+1].transpose(1, 0, 2, 3)     # (channels, 1, n, n)\n",
    "      grad_out = dL_dz[b:b+1]                           # (1, num_kernels, n, n)\n",
    "\n",
    "      grad = self.conv2d(grad_out, input_, padding_valid=self.padding_valid)  # (1, num_kernels, k, k)\n",
    "\n",
    "      dL_dk += grad.transpose(1, 0, 2, 3)               # Convert back to (num_kernels, channels, k, k)\n",
    "\n",
    "    # Average gradients over batch size\n",
    "    dL_dk /= X_batch.shape[0]\n",
    "\n",
    "    # Update kernel and bias\n",
    "    self.kernel -= lr * dL_dk\n",
    "    self.bias -= lr * dL_db\n",
    "\n",
    "  def get_batches(self, X, y, batch_size):\n",
    "    for i in range(0, len(X), batch_size):\n",
    "      yield X[i:i+batch_size], y[i:i+batch_size]\n",
    "\n",
    "  def train_and_test(self, X_train, y_train, X_test, y_test, epochs=100, lr=0.1, batch_size=32):\n",
    "    for epoch in range(epochs):\n",
    "      total_loss = 0\n",
    "      batches = self.nn.get_batches(X_train, y_train, batch_size)\n",
    "\n",
    "      for X_batch, y_batch in batches:\n",
    "        Z, A, A_pool, A_flat, nn_zs, nn_As = self.forward(X_batch)\n",
    "        loss = self.nn.loss_fn.f(y_batch, nn_As[-1])\n",
    "        total_loss += loss\n",
    "        self.backward(X_batch, y_batch, Z, A, A_pool, A_flat, nn_zs, nn_As, lr)\n",
    "\n",
    "      avg_loss = total_loss / (len(X_train) // batch_size)\n",
    "\n",
    "      # Test after each epoch\n",
    "      test_activations = self.forward(X_test)[-1]\n",
    "      y_pred = np.argmax(test_activations[-1], axis=1)\n",
    "      accuracy = np.mean(y_pred == y_test)\n",
    "      \n",
    "      print(f\"Epoch {epoch}, Loss: {avg_loss:.4f}, Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5b871195",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 1, 28, 28),\n",
       " (10000, 1, 28, 28),\n",
       " (60000,),\n",
       " (10000,),\n",
       " ['T-shirt/top',\n",
       "  'Trouser',\n",
       "  'Pullover',\n",
       "  'Dress',\n",
       "  'Coat',\n",
       "  'Sandal',\n",
       "  'Shirt',\n",
       "  'Sneaker',\n",
       "  'Bag',\n",
       "  'Ankle boot'])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load FashionMNIST dataset from pytorch\n",
    "train_dataset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=ToTensor())\n",
    "test_dataset = datasets.FashionMNIST(root=\"./data\", train=False, download=True, transform=ToTensor())\n",
    "class_names = train_dataset.classes\n",
    "\n",
    "# Extract data and labels\n",
    "X_train = train_dataset.data.numpy().astype(np.float32) / 255.0 # Normalize to [0, 1]\n",
    "y_train = train_dataset.targets.numpy()\n",
    "X_test = test_dataset.data.numpy().astype(np.float32) / 255.0 # Normalize to [0, 1]\n",
    "y_test = test_dataset.targets.numpy()\n",
    "\n",
    "X_train = X_train.reshape(-1, 1, 28, 28)  # Reshape to (batch_size, channels, height, width)\n",
    "X_test = X_test.reshape(-1, 1, 28, 28)    # Reshape to (batch_size, channels, height, width)\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape, class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65912b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.5590, Test Accuracy: 79.90%\n",
      "Epoch 1, Loss: 0.3970, Test Accuracy: 83.15%\n",
      "Epoch 2, Loss: 0.3549, Test Accuracy: 83.17%\n",
      "Epoch 3, Loss: 0.3279, Test Accuracy: 85.46%\n",
      "Epoch 4, Loss: 0.3091, Test Accuracy: 86.13%\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Train and test the model\n",
    "model = CNN_Multiple_Filters(input_shape=X_train.shape[1:], \n",
    "                                  nn_hidden_units=[128, 64], \n",
    "                                  output_shape=10, \n",
    "                                  kernel_size=3, \n",
    "                                  stride=1, \n",
    "                                  padding_valid=False,\n",
    "                                  num_kernels=4,\n",
    "                                  pool_size=2, \n",
    "                                  pool_stride=2)\n",
    "model.train_and_test(X_train, y_train, X_test, y_test, epochs=5, lr=0.1, batch_size=32)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataScienceClass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
