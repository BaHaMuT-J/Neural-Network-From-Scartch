{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b3a279e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85aaf38d",
   "metadata": {},
   "source": [
    "## Fully Connected Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44a4ba8",
   "metadata": {},
   "source": [
    "### Activation and Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec221ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "  def f(self, x):\n",
    "    return np.maximum(0, x)\n",
    "  \n",
    "  def fp(self, x):\n",
    "    return (x > 0).astype(float)\n",
    "  \n",
    "# Empty activation\n",
    "class EmptyActivation:\n",
    "  def f(self, x):\n",
    "    return x\n",
    "\n",
    "  def fp(self, x):\n",
    "    return np.ones_like(x)\n",
    "\n",
    "class SoftmaxCrossEntropy:\n",
    "    def f(self, y_true, logits):\n",
    "        # logits: shape (batch_size, num_classes)\n",
    "        # y_true: shape (batch_size,) with class indices\n",
    "\n",
    "        # Compute softmax\n",
    "        exps = np.exp(logits - np.max(logits, axis=1, keepdims=True))\n",
    "        probs = exps / np.sum(exps, axis=1, keepdims=True)\n",
    "        self.probs = probs\n",
    "\n",
    "        # Cross-entropy loss\n",
    "        batch_size = y_true.shape[0]\n",
    "        correct_logprobs = -np.log(probs[range(batch_size), y_true])\n",
    "        return np.mean(correct_logprobs)\n",
    "\n",
    "    def fp(self, y_true, logits):\n",
    "        # Derivative: probs - y_true_onehot\n",
    "        batch_size = y_true.shape[0]\n",
    "        grad = self.probs.copy()\n",
    "        grad[range(batch_size), y_true] -= 1\n",
    "        return grad / batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3727b39",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ab1c7f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified to be used with CNN\n",
    "class NeuralNetworkForCNN:\n",
    "  def __init__(self, input_shape: int, hidden_units: list[int], output_shape: int):\n",
    "    self.l = len(hidden_units) + 1  # number of layers\n",
    "    self.W = []                     # weights\n",
    "    self.b = []                     # biases\n",
    "    self.a = []                     # activations\n",
    "    \n",
    "    prev_units = input_shape\n",
    "    for units in hidden_units:\n",
    "      weight = np.random.randn(prev_units, units) * np.sqrt(2 / prev_units)\n",
    "      bias = np.zeros((1, units))\n",
    "      self.W.append(weight)\n",
    "      self.b.append(bias)\n",
    "      self.a.append(ReLU())\n",
    "      prev_units = units\n",
    "\n",
    "    # Output layer\n",
    "    weight = np.random.randn(prev_units, output_shape) * np.sqrt(2 / prev_units)\n",
    "    bias = np.zeros((1, output_shape))\n",
    "    self.W.append(weight)\n",
    "    self.b.append(bias)\n",
    "    self.a.append(EmptyActivation())\n",
    "\n",
    "    self.loss_fn = SoftmaxCrossEntropy()\n",
    "  \n",
    "  def set_activation(self, a: list):\n",
    "    self.a = a\n",
    "\n",
    "  def set_loss_fn(self, L):\n",
    "    self.loss_fn = L\n",
    "  \n",
    "  def forward(self, X):\n",
    "    x = X.copy()\n",
    "    zs = []  # pre-activation\n",
    "    As = []  # post-activation\n",
    "    for i in range(self.l):\n",
    "      z = x @ self.W[i] + self.b[i]\n",
    "      zs.append(z)\n",
    "      A = self.a[i].f(z)\n",
    "      As.append(A)\n",
    "      x = A\n",
    "    return zs, As\n",
    "  \n",
    "  def backward(self, X, y, zs, As, lr=0.01):\n",
    "    # Partial derivative of loss w.r.t. output\n",
    "    dL_dO = self.loss_fn.fp(y, As[-1])\n",
    "\n",
    "    # Gradient of layer i+1 w.r.t. layer i\n",
    "    dA = dL_dO\n",
    "    for i in range(self.l-1, -1, -1):\n",
    "      # Partial derivative of loss w.r.t. activation\n",
    "      a_deriv = self.a[i].fp(zs[i])\n",
    "      dL_dz = dA * a_deriv\n",
    "\n",
    "      # Update gradient for layer i-1\n",
    "      dA = dL_dz @ self.W[i].T\n",
    "\n",
    "      # Partial derivative of loss w.r.t. bias\n",
    "      dL_db = np.sum(dL_dz, axis=0, keepdims=True)\n",
    "\n",
    "      # Partial derivative of loss w.r.t. weight\n",
    "      dz_dW = As[i-1] if i > 0 else X\n",
    "      dL_dW = dz_dW.T @ dL_dz\n",
    "\n",
    "      # Update weights and biases\n",
    "      self.b[i] -= lr * dL_db\n",
    "      self.W[i] -= lr * dL_dW\n",
    "    return dA\n",
    "  \n",
    "  def get_batches(self, X, y, batch_size):\n",
    "    for i in range(0, len(X), batch_size):\n",
    "      yield X[i:i+batch_size], y[i:i+batch_size]\n",
    "\n",
    "\n",
    "  def train(self, X, y, epochs=100, lr=0.01, batch_size=32):\n",
    "    for epoch in range(epochs):\n",
    "      total_loss = 0\n",
    "      batches = self.get_batches(X, y, batch_size)\n",
    "\n",
    "      for X_batch, y_batch in batches:\n",
    "        zs, As = self.forward(X_batch)\n",
    "        loss = self.loss_fn.f(y_batch, As[-1])\n",
    "        total_loss += loss\n",
    "        self.backward(X_batch, y_batch, zs, As, lr)\n",
    "\n",
    "      avg_loss = total_loss / (len(X) // batch_size)\n",
    "\n",
    "      if epoch % 1 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300e3d84",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1bc857be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 784),\n",
       " (10000, 784),\n",
       " (60000,),\n",
       " (10000,),\n",
       " ['T-shirt/top',\n",
       "  'Trouser',\n",
       "  'Pullover',\n",
       "  'Dress',\n",
       "  'Coat',\n",
       "  'Sandal',\n",
       "  'Shirt',\n",
       "  'Sneaker',\n",
       "  'Bag',\n",
       "  'Ankle boot'])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load FashionMNIST dataset from pytorch\n",
    "train_dataset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=ToTensor())\n",
    "test_dataset = datasets.FashionMNIST(root=\"./data\", train=False, download=True, transform=ToTensor())\n",
    "class_names = train_dataset.classes\n",
    "\n",
    "# Extract data and labels\n",
    "X_train = train_dataset.data.numpy().reshape(-1, 28*28).astype(np.float32) / 255.0 # Normalize to [0, 1]\n",
    "y_train = train_dataset.targets.numpy()\n",
    "X_test = test_dataset.data.numpy().reshape(-1, 28*28).astype(np.float32) / 255.0 # Normalize to [0, 1]\n",
    "y_test = test_dataset.targets.numpy()\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape, class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "74fa995a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.5369\n",
      "Epoch 1, Loss: 0.3908\n",
      "Epoch 2, Loss: 0.3513\n",
      "Epoch 3, Loss: 0.3263\n",
      "Epoch 4, Loss: 0.3079\n",
      "Epoch 5, Loss: 0.2924\n",
      "Epoch 6, Loss: 0.2792\n",
      "Epoch 7, Loss: 0.2679\n",
      "Epoch 8, Loss: 0.2584\n",
      "Epoch 9, Loss: 0.2503\n",
      "Epoch 10, Loss: 0.2417\n",
      "Epoch 11, Loss: 0.2360\n",
      "Epoch 12, Loss: 0.2309\n",
      "Epoch 13, Loss: 0.2232\n",
      "Epoch 14, Loss: 0.2182\n",
      "Epoch 15, Loss: 0.2106\n",
      "Epoch 16, Loss: 0.2062\n",
      "Epoch 17, Loss: 0.2019\n",
      "Epoch 18, Loss: 0.1966\n",
      "Epoch 19, Loss: 0.1914\n",
      "Epoch 20, Loss: 0.1846\n",
      "Epoch 21, Loss: 0.1815\n",
      "Epoch 22, Loss: 0.1783\n",
      "Epoch 23, Loss: 0.1768\n",
      "Epoch 24, Loss: 0.1761\n",
      "Epoch 25, Loss: 0.1719\n",
      "Epoch 26, Loss: 0.1665\n",
      "Epoch 27, Loss: 0.1621\n",
      "Epoch 28, Loss: 0.1606\n",
      "Epoch 29, Loss: 0.1561\n",
      "Epoch 30, Loss: 0.1539\n",
      "Epoch 31, Loss: 0.1516\n",
      "Epoch 32, Loss: 0.1479\n",
      "Epoch 33, Loss: 0.1485\n",
      "Epoch 34, Loss: 0.1420\n",
      "Epoch 35, Loss: 0.1435\n",
      "Epoch 36, Loss: 0.1375\n",
      "Epoch 37, Loss: 0.1373\n",
      "Epoch 38, Loss: 0.1347\n",
      "Epoch 39, Loss: 0.1309\n",
      "Epoch 40, Loss: 0.1272\n",
      "Epoch 41, Loss: 0.1330\n",
      "Epoch 42, Loss: 0.1240\n",
      "Epoch 43, Loss: 0.1211\n",
      "Epoch 44, Loss: 0.1188\n",
      "Epoch 45, Loss: 0.1221\n",
      "Epoch 46, Loss: 0.1229\n",
      "Epoch 47, Loss: 0.1231\n",
      "Epoch 48, Loss: 0.1206\n",
      "Epoch 49, Loss: 0.1246\n",
      "Epoch 50, Loss: 0.1164\n",
      "Epoch 51, Loss: 0.1121\n",
      "Epoch 52, Loss: 0.1077\n",
      "Epoch 53, Loss: 0.1052\n",
      "Epoch 54, Loss: 0.1074\n",
      "Epoch 55, Loss: 0.1048\n",
      "Epoch 56, Loss: 0.1072\n",
      "Epoch 57, Loss: 0.1081\n",
      "Epoch 58, Loss: 0.1095\n",
      "Epoch 59, Loss: 0.1015\n",
      "Epoch 60, Loss: 0.1021\n",
      "Epoch 61, Loss: 0.0966\n",
      "Epoch 62, Loss: 0.1011\n",
      "Epoch 63, Loss: 0.1018\n",
      "Epoch 64, Loss: 0.1013\n",
      "Epoch 65, Loss: 0.1037\n",
      "Epoch 66, Loss: 0.0939\n",
      "Epoch 67, Loss: 0.0976\n",
      "Epoch 68, Loss: 0.0957\n",
      "Epoch 69, Loss: 0.0944\n",
      "Epoch 70, Loss: 0.0930\n",
      "Epoch 71, Loss: 0.0928\n",
      "Epoch 72, Loss: 0.0874\n",
      "Epoch 73, Loss: 0.0850\n",
      "Epoch 74, Loss: 0.0919\n",
      "Epoch 75, Loss: 0.0854\n",
      "Epoch 76, Loss: 0.0895\n",
      "Epoch 77, Loss: 0.0784\n",
      "Epoch 78, Loss: 0.0820\n",
      "Epoch 79, Loss: 0.0864\n",
      "Epoch 80, Loss: 0.0798\n",
      "Epoch 81, Loss: 0.0814\n",
      "Epoch 82, Loss: 0.0881\n",
      "Epoch 83, Loss: 0.0838\n",
      "Epoch 84, Loss: 0.0886\n",
      "Epoch 85, Loss: 0.0811\n",
      "Epoch 86, Loss: 0.0787\n",
      "Epoch 87, Loss: 0.0728\n",
      "Epoch 88, Loss: 0.0767\n",
      "Epoch 89, Loss: 0.0818\n",
      "Epoch 90, Loss: 0.0740\n",
      "Epoch 91, Loss: 0.0726\n",
      "Epoch 92, Loss: 0.0757\n",
      "Epoch 93, Loss: 0.0732\n",
      "Epoch 94, Loss: 0.0710\n",
      "Epoch 95, Loss: 0.0778\n",
      "Epoch 96, Loss: 0.0720\n",
      "Epoch 97, Loss: 0.0707\n",
      "Epoch 98, Loss: 0.0680\n",
      "Epoch 99, Loss: 0.0759\n",
      "Test Accuracy: 87.44%\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "model = NeuralNetworkForCNN(input_shape=784, hidden_units=[128, 64], output_shape=10)\n",
    "\n",
    "# Train\n",
    "model.train(X_train, y_train, epochs=100, lr=0.1)\n",
    "\n",
    "# Test\n",
    "_, test_activations = model.forward(X_test)\n",
    "y_pred = np.argmax(test_activations[-1], axis=1)\n",
    "accuracy = np.mean(y_pred == y_test)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038089b6",
   "metadata": {},
   "source": [
    "## CNN Without Padding and Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3963a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_NoPaddingNoPooling:\n",
    "  def __init__(self, input_shape, nn_hidden_units, output_shape, kernel_size, stride=1):\n",
    "    self.stride = stride # Use stride=1 for now\n",
    "    cnn_shape = ((input_shape - kernel_size) // self.stride) + 1 # Final shape after convolution\n",
    "    \n",
    "    self.kernel = np.random.randn(kernel_size, kernel_size) * np.sqrt(2 / (kernel_size * kernel_size))\n",
    "    self.bias = np.zeros((cnn_shape, cnn_shape))\n",
    "    self.activation = ReLU()\n",
    "    self.nn = NeuralNetworkForCNN(input_shape=cnn_shape**2, hidden_units=nn_hidden_units, output_shape=output_shape)\n",
    "    pass\n",
    "\n",
    "  # Only 2D convolution is implemented\n",
    "  def conv2d(self, X_batch, kernel):\n",
    "    batch_size, n, _ = X_batch.shape\n",
    "    k = kernel.shape[0]\n",
    "    final_shape = ((n - k) // self.stride) + 1\n",
    "    output = np.zeros((batch_size, final_shape, final_shape))\n",
    "\n",
    "    for i in range(0, final_shape, self.stride):\n",
    "      for j in range(0, final_shape, self.stride):\n",
    "        region = X_batch[:, i:i+k, j:j+k]                       # shape: (batch_size, k, k)\n",
    "        output[:, i, j] = np.sum(region * kernel, axis=(1, 2))  # sum over k x k\n",
    "\n",
    "    return output\n",
    "\n",
    "  def forward(self, X_batch):\n",
    "    Z = self.conv2d(X_batch, self.kernel) + self.bias\n",
    "    A = self.activation.f(Z)\n",
    "    A_flat = A.reshape(A.shape[0], -1)  # flatten each sample\n",
    "    nn_zs, nn_As = self.nn.forward(A_flat)\n",
    "    return Z, A, A_flat, nn_zs, nn_As\n",
    "  \n",
    "  def backward(self, X_batch, y_batch, z, A, A_flat, nn_zs, nn_As, lr=0.1):\n",
    "    dL_dAF = self.nn.backward(A_flat, y_batch, nn_zs, nn_As, lr)\n",
    "    dL_dA = dL_dAF.reshape(A.shape)               # Reshape back to original shape\n",
    "\n",
    "    dA_dz = self.activation.fp(z)                 # Gradient of activation function\n",
    "    dL_dz = dL_dA * dA_dz                         # Gradient of loss w.r.t. z\n",
    "\n",
    "    # Gradient of loss w.r.t. bias\n",
    "    dL_db = np.sum(dL_dz, axis=0)\n",
    "\n",
    "    # Convolution backward: convolve input with dL_dz\n",
    "    dL_dk = np.zeros_like(self.kernel)\n",
    "    for b in range(X_batch.shape[0]):\n",
    "      dL_dk += self.conv2d(X_batch[b:b+1], dL_dz[b]).squeeze()\n",
    "\n",
    "    # Average gradients over batch size\n",
    "    dL_dk /= X_batch.shape[0]\n",
    "\n",
    "    # Update kernel and bias\n",
    "    self.kernel -= lr * dL_dk\n",
    "    self.bias -= lr * dL_db\n",
    "\n",
    "  def get_batches(self, X, y, batch_size):\n",
    "    for i in range(0, len(X), batch_size):\n",
    "      yield X[i:i+batch_size], y[i:i+batch_size]\n",
    "\n",
    "  def train(self, X, y, epochs=100, lr=0.1, batch_size=32):\n",
    "    for epoch in range(epochs):\n",
    "      total_loss = 0\n",
    "      batches = self.nn.get_batches(X, y, batch_size)\n",
    "\n",
    "      for X_batch, y_batch in batches:\n",
    "        Z, A, A_flat, nn_zs, nn_As = self.forward(X_batch)\n",
    "        loss = self.nn.loss_fn.f(y_batch, nn_As[-1])\n",
    "        total_loss += loss\n",
    "        self.backward(X_batch, y_batch, Z, A, A_flat, nn_zs, nn_As, lr)\n",
    "\n",
    "      avg_loss = total_loss / (len(X) // batch_size)\n",
    "      print(f\"Epoch {epoch}, Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "472e4cd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 28, 28),\n",
       " (10000, 28, 28),\n",
       " (60000,),\n",
       " (10000,),\n",
       " ['T-shirt/top',\n",
       "  'Trouser',\n",
       "  'Pullover',\n",
       "  'Dress',\n",
       "  'Coat',\n",
       "  'Sandal',\n",
       "  'Shirt',\n",
       "  'Sneaker',\n",
       "  'Bag',\n",
       "  'Ankle boot'])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load FashionMNIST dataset from pytorch\n",
    "train_dataset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=ToTensor())\n",
    "test_dataset = datasets.FashionMNIST(root=\"./data\", train=False, download=True, transform=ToTensor())\n",
    "class_names = train_dataset.classes\n",
    "\n",
    "# Extract data and labels\n",
    "X_train = train_dataset.data.numpy().astype(np.float32) / 255.0 # Normalize to [0, 1]\n",
    "y_train = train_dataset.targets.numpy()\n",
    "X_test = test_dataset.data.numpy().astype(np.float32) / 255.0 # Normalize to [0, 1]\n",
    "y_test = test_dataset.targets.numpy()\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape, class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d629c24f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.6251\n",
      "Epoch 1, Loss: 0.4949\n",
      "Epoch 2, Loss: 0.4690\n",
      "Epoch 3, Loss: 0.4530\n",
      "Epoch 4, Loss: 0.4410\n",
      "Epoch 5, Loss: 0.4310\n",
      "Epoch 6, Loss: 0.4224\n",
      "Epoch 7, Loss: 0.4150\n",
      "Epoch 8, Loss: 0.4085\n",
      "Epoch 9, Loss: 0.4027\n",
      "Epoch 10, Loss: 0.3975\n",
      "Epoch 11, Loss: 0.3928\n",
      "Epoch 12, Loss: 0.3887\n",
      "Epoch 13, Loss: 0.3849\n",
      "Epoch 14, Loss: 0.3814\n",
      "Epoch 15, Loss: 0.3781\n",
      "Epoch 16, Loss: 0.3751\n",
      "Epoch 17, Loss: 0.3723\n",
      "Epoch 18, Loss: 0.3697\n",
      "Epoch 19, Loss: 0.3673\n",
      "Test Accuracy: 85.44%\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Train\n",
    "model = CNN_NoPaddingNoPooling(input_shape=28, nn_hidden_units=[], output_shape=10, kernel_size=3, stride=1)\n",
    "model.train(X_train, y_train, epochs=20, lr=0.1, batch_size=32)\n",
    "\n",
    "# Test\n",
    "_, _, _, _, test_activations = model.forward(X_test)\n",
    "y_pred = np.argmax(test_activations[-1], axis=1)\n",
    "accuracy = np.mean(y_pred == y_test)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataScienceClass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
