{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b80f8d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e95f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 113.9959\n",
      "Epoch 100, Loss: 8.4656\n",
      "Epoch 200, Loss: 8.4656\n",
      "Epoch 300, Loss: 8.4656\n",
      "Epoch 400, Loss: 8.4656\n",
      "Epoch 500, Loss: 8.4656\n",
      "Epoch 600, Loss: 8.4656\n",
      "Epoch 700, Loss: 8.4656\n",
      "Epoch 800, Loss: 8.4656\n",
      "Epoch 900, Loss: 8.4656\n",
      "\n",
      "Final Weights:\n",
      "W1: [[1.59530964]]\n",
      "W2: [[1.59530964]]\n",
      "\n",
      "Prediction:\n",
      " [[ 0.        ]\n",
      " [ 2.54501285]\n",
      " [ 5.09002571]\n",
      " [ 7.63503856]\n",
      " [10.18005141]\n",
      " [12.72506427]\n",
      " [15.27007712]\n",
      " [17.81508997]\n",
      " [20.36010282]\n",
      " [22.90511568]]\n",
      "\n",
      "y True:\n",
      " [[ 0]\n",
      " [ 2]\n",
      " [ 4]\n",
      " [ 6]\n",
      " [ 8]\n",
      " [10]\n",
      " [12]\n",
      " [14]\n",
      " [16]\n",
      " [18]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Goal y = 2x\n",
    "z = np.array([np.arange(0, 10)]).T\n",
    "y = 2 * z\n",
    "\n",
    "# Initialize params, has 1 hidden layers with 1 neurons, no bias\n",
    "def init_params(input_shape, output_shape):\n",
    "  W1 = np.random.rand(input_shape, input_shape) * 0.01\n",
    "  W2 = np.random.rand(input_shape, output_shape) * 0.01\n",
    "  return W1, W2\n",
    "\n",
    "# Forward propagation\n",
    "def forward(x, W1, W2):\n",
    "  hidden =  x @ W1\n",
    "  output = hidden @ W2\n",
    "  return hidden, output\n",
    "\n",
    "# MSE loss\n",
    "def MSE(y_true, y_pred):\n",
    "  return np.mean(np.square(y_true-y_pred))\n",
    "\n",
    "# Backward propagation\n",
    "def backward(x, y, W1, W2, H, O, lr=0.01):\n",
    "  dL_dO = 2 * (O - y) / O.shape[0]\n",
    "  \n",
    "  dO_dW2 = H\n",
    "  dL_dW2 = dO_dW2.T @ dL_dO\n",
    "  \n",
    "  dO_dH = W2\n",
    "  dH_dW1 = x\n",
    "  dL_dW1 = dH_dW1.T @ (dL_dO @ dO_dH.T)\n",
    "\n",
    "  W2 -= lr * dL_dW2\n",
    "  W1 -= lr * dL_dW1\n",
    "\n",
    "  return W1, W2\n",
    "\n",
    "# Training loop\n",
    "epochs = 1000\n",
    "W1, W2 = init_params(1, 1)\n",
    "for epoch in range(epochs):\n",
    "  A, O = forward(z, W1, W2)\n",
    "\n",
    "  if epoch % 100 == 0:\n",
    "    loss = MSE(y, O)\n",
    "    print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "\n",
    "  W1, W2 = backward(z, y, W1, W2, A, O)\n",
    "\n",
    "print(\"\\nFinal Weights:\")\n",
    "print(\"W1:\", W1)\n",
    "print(\"W2:\", W2)\n",
    "print(\"\\nPrediction:\\n\", forward(z, W1, W2)[1])\n",
    "print(\"\\ny True:\\n\", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb1796f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 162.8282\n",
      "Epoch 100, Loss: 10.3533\n",
      "Epoch 200, Loss: 7.2105\n",
      "Epoch 300, Loss: 5.1797\n",
      "Epoch 400, Loss: 3.7506\n",
      "Epoch 500, Loss: 2.7163\n",
      "Epoch 600, Loss: 1.9633\n",
      "Epoch 700, Loss: 1.4154\n",
      "Epoch 800, Loss: 1.0179\n",
      "Epoch 900, Loss: 0.7302\n",
      "\n",
      "Final Params:\n",
      "W1: [[1.54516413]] | b1: -1.2619867906488234\n",
      "W2: [[1.38274399]] | b2: 4.737832771104995\n",
      "\n",
      "Prediction:\n",
      " [[ 2.99282812]\n",
      " [ 5.12939453]\n",
      " [ 7.26596094]\n",
      " [ 9.40252735]\n",
      " [11.53909375]\n",
      " [13.67566016]\n",
      " [15.81222657]\n",
      " [17.94879298]\n",
      " [20.08535939]\n",
      " [22.22192579]]\n",
      "\n",
      "y True:\n",
      " [[ 3]\n",
      " [ 5]\n",
      " [ 7]\n",
      " [ 9]\n",
      " [11]\n",
      " [13]\n",
      " [15]\n",
      " [17]\n",
      " [19]\n",
      " [21]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Goal y = 2x + 3\n",
    "z = np.array([np.arange(0, 10)]).T\n",
    "y = 2 * z + 3\n",
    "\n",
    "# Initialize params, has 1 hidden layers with 1 neurons, with bias\n",
    "def init_params(input_shape, output_shape):\n",
    "  W1 = np.random.rand(input_shape, input_shape) * 0.01\n",
    "  b1 = np.random.random()\n",
    "  W2 = np.random.rand(input_shape, output_shape) * 0.01\n",
    "  b2 = np.random.random()\n",
    "  return W1, b1, W2, b2\n",
    "\n",
    "# Forward propagation\n",
    "def forward(x, W1, b1, W2, b2):\n",
    "  hidden =  x @ W1 + b1\n",
    "  output = hidden @ W2 + b2\n",
    "  return hidden, output\n",
    "\n",
    "# MSE loss\n",
    "def MSE(y_true, y_pred):\n",
    "  return np.mean(np.square(y_true-y_pred))\n",
    "\n",
    "# Backward propagation\n",
    "def backward(x, y, W1, b1, W2, b2, H, O, lr=0.01):\n",
    "  dL_dO = 2 * (O - y) / O.shape[0]\n",
    "\n",
    "  dL_db2 = np.sum(dL_dO)\n",
    "  \n",
    "  dO_dW2 = H\n",
    "  dL_dW2 = dO_dW2.T @ dL_dO\n",
    "  \n",
    "  dO_dH = W2\n",
    "  dL_dH = dL_dO @ dO_dH.T\n",
    "  dL_db1 = np.sum(dL_dH)\n",
    "\n",
    "  dH_dW1 = x\n",
    "  dL_dW1 = dH_dW1.T @ (dL_dH)\n",
    "\n",
    "  b2 -= lr * dL_db2\n",
    "  W2 -= lr * dL_dW2\n",
    "  b1 -= lr * dL_db1\n",
    "  W1 -= lr * dL_dW1\n",
    "\n",
    "  return W1, b1, W2, b2\n",
    "\n",
    "# Training loop\n",
    "epochs = 1000\n",
    "W1, b1, W2, b2 = init_params(1, 1)\n",
    "for epoch in range(epochs):\n",
    "  A, O = forward(z, W1, b1, W2, b2)\n",
    "\n",
    "  if epoch % 100 == 0:\n",
    "    loss = MSE(y, O)\n",
    "    print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "\n",
    "  W1, b1, W2, b2 = backward(z, y, W1, b1, W2, b2, A, O)\n",
    "\n",
    "print(\"\\nFinal Params:\")\n",
    "print(f\"W1: {W1} | b1: {b1}\")\n",
    "print(f\"W2: {W2} | b2: {b2}\")\n",
    "print(\"\\nPrediction:\\n\", forward(z, W1, b1, W2, b2)[1])\n",
    "print(\"\\ny True:\\n\", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b88666c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 114.2859\n",
      "Epoch 100, Loss: 11.4438\n",
      "Epoch 200, Loss: 7.7742\n",
      "Epoch 300, Loss: 5.7047\n",
      "Epoch 400, Loss: 4.6576\n",
      "Epoch 500, Loss: 4.0356\n",
      "Epoch 600, Loss: 3.6277\n",
      "Epoch 700, Loss: 3.3509\n",
      "Epoch 800, Loss: 3.1612\n",
      "Epoch 900, Loss: 3.0309\n",
      "Epoch 1000, Loss: 2.9412\n",
      "Epoch 1100, Loss: 2.8793\n",
      "Epoch 1200, Loss: 2.8365\n",
      "Epoch 1300, Loss: 2.8067\n",
      "Epoch 1400, Loss: 2.7861\n",
      "Epoch 1500, Loss: 2.7717\n",
      "Epoch 1600, Loss: 2.7617\n",
      "Epoch 1700, Loss: 2.7547\n",
      "Epoch 1800, Loss: 2.7498\n",
      "Epoch 1900, Loss: 2.7464\n",
      "Epoch 2000, Loss: 2.7441\n",
      "Epoch 2100, Loss: 2.7424\n",
      "Epoch 2200, Loss: 2.7412\n",
      "Epoch 2300, Loss: 2.7404\n",
      "Epoch 2400, Loss: 2.7399\n",
      "Epoch 2500, Loss: 2.7395\n",
      "Epoch 2600, Loss: 2.7392\n",
      "Epoch 2700, Loss: 2.7390\n",
      "Epoch 2800, Loss: 2.7389\n",
      "Epoch 2900, Loss: 2.7388\n",
      "\n",
      "Final Params:\n",
      "W1: [[1.6121592]] | b1: -1.0314457218900595\n",
      "W2: [[1.43381945]] | b2: 4.3973232885666365\n",
      "\n",
      "Prediction:\n",
      " [[ 4.39732329]\n",
      " [ 5.22996157]\n",
      " [ 7.5415068 ]\n",
      " [ 9.85305203]\n",
      " [12.16459725]\n",
      " [14.47614248]\n",
      " [16.7876877 ]\n",
      " [19.09923293]\n",
      " [21.41077816]\n",
      " [23.72232338]]\n",
      "\n",
      "y True:\n",
      " [[ 3]\n",
      " [ 5]\n",
      " [ 7]\n",
      " [ 9]\n",
      " [11]\n",
      " [13]\n",
      " [15]\n",
      " [17]\n",
      " [19]\n",
      " [21]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Goal y = 2x + 3\n",
    "x = np.array([np.arange(0, 10)]).T\n",
    "y = 2 * x + 3\n",
    "\n",
    "# Initialize params, has 1 hidden layers with 1 neurons, with bias\n",
    "def init_params(input_shape, output_shape):\n",
    "  W1 = np.random.rand(input_shape, input_shape)\n",
    "  b1 = np.random.random()\n",
    "  W2 = np.random.rand(input_shape, output_shape)\n",
    "  b2 = np.random.random()\n",
    "  return W1, b1, W2, b2\n",
    "\n",
    "def relu(z):\n",
    "  return np.maximum(0, z)\n",
    "\n",
    "def relu_deriv(z):\n",
    "  return (z > 0).astype(float)\n",
    "\n",
    "# Forward propagation, now using ReLU as activation\n",
    "def forward(x, W1, b1, W2, b2):\n",
    "  a =  x @ W1 + b1\n",
    "  A = relu(a)\n",
    "  z = A @ W2 + b2\n",
    "  O = relu(z)\n",
    "  return a, A, z, O\n",
    "\n",
    "# MSE loss\n",
    "def MSE(y_true, y_pred):\n",
    "  return np.mean(np.square(y_true-y_pred))\n",
    "\n",
    "# Backward propagation\n",
    "def backward(x, y, W1, b1, W2, b2, a, A, z, O, lr=0.01):\n",
    "  dL_dO = 2 * (O - y) / O.shape[0]\n",
    "  dO_dz = relu_deriv(z)\n",
    "  dL_dz = dL_dO * dO_dz\n",
    "\n",
    "  dL_db2 = np.sum(dL_dz)\n",
    "  \n",
    "  dz_dW2 = A\n",
    "  dL_dW2 = dz_dW2.T @ dL_dz\n",
    "  \n",
    "  dz_dA = W2\n",
    "  dL_dA = dL_dz @ dz_dA.T\n",
    "\n",
    "  dA_da = relu_deriv(a)\n",
    "  dL_da = dL_dA * dA_da\n",
    "  dL_db1 = np.sum(dL_da)\n",
    "  \n",
    "  da_dw1 = x\n",
    "  dL_dW1 = da_dw1.T @ (dL_da)\n",
    "\n",
    "  b2 -= lr * dL_db2\n",
    "  W2 -= lr * dL_dW2\n",
    "  b1 -= lr * dL_db1\n",
    "  W1 -= lr * dL_dW1\n",
    "\n",
    "  return W1, b1, W2, b2\n",
    "\n",
    "# Training loop\n",
    "epochs = 3000\n",
    "W1, b1, W2, b2 = init_params(1, 1)\n",
    "for epoch in range(epochs):\n",
    "  a, A, z, O = forward(x, W1, b1, W2, b2)\n",
    "\n",
    "  if epoch % 100 == 0:\n",
    "    loss = MSE(y, O)\n",
    "    print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "\n",
    "  W1, b1, W2, b2 = backward(x, y, W1, b1, W2, b2, a, A, z, O)\n",
    "\n",
    "print(\"\\nFinal Params:\")\n",
    "print(f\"W1: {W1} | b1: {b1}\")\n",
    "print(f\"W2: {W2} | b2: {b2}\")\n",
    "print(\"\\nPrediction:\\n\", forward(x, W1, b1, W2, b2)[3])\n",
    "print(\"\\ny True:\\n\", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "61565259",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "  def f(self, x):\n",
    "    return np.maximum(0, x)\n",
    "  \n",
    "  def fp(self, x):\n",
    "    return (x > 0).astype(float)\n",
    "  \n",
    "class MSE:\n",
    "  def f(self, y_true, y_pred):\n",
    "    return np.mean(np.square(y_true-y_pred))\n",
    "  \n",
    "  def fp(self, y_true, y_pred):\n",
    "    return 2 * (y_pred - y_true) / y_pred.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23178e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 114.2859\n",
      "Epoch 100, Loss: 11.4438\n",
      "Epoch 200, Loss: 7.7742\n",
      "Epoch 300, Loss: 5.7047\n",
      "Epoch 400, Loss: 4.6576\n",
      "Epoch 500, Loss: 4.0356\n",
      "Epoch 600, Loss: 3.6277\n",
      "Epoch 700, Loss: 3.3509\n",
      "Epoch 800, Loss: 3.1612\n",
      "Epoch 900, Loss: 3.0309\n",
      "Epoch 1000, Loss: 2.9412\n",
      "Epoch 1100, Loss: 2.8793\n",
      "Epoch 1200, Loss: 2.8365\n",
      "Epoch 1300, Loss: 2.8067\n",
      "Epoch 1400, Loss: 2.7861\n",
      "Epoch 1500, Loss: 2.7717\n",
      "Epoch 1600, Loss: 2.7617\n",
      "Epoch 1700, Loss: 2.7547\n",
      "Epoch 1800, Loss: 2.7498\n",
      "Epoch 1900, Loss: 2.7464\n",
      "Epoch 2000, Loss: 2.7441\n",
      "Epoch 2100, Loss: 2.7424\n",
      "Epoch 2200, Loss: 2.7412\n",
      "Epoch 2300, Loss: 2.7404\n",
      "Epoch 2400, Loss: 2.7399\n",
      "Epoch 2500, Loss: 2.7395\n",
      "Epoch 2600, Loss: 2.7392\n",
      "Epoch 2700, Loss: 2.7390\n",
      "Epoch 2800, Loss: 2.7389\n",
      "Epoch 2900, Loss: 2.7388\n",
      "\n",
      "Final Params:\n",
      "W1: [[1.6121592]] | b1: -1.0314457218900595\n",
      "W2: [[1.43381945]] | b2: 4.3973232885666365\n",
      "\n",
      "Prediction:\n",
      " [[ 4.39732329]\n",
      " [ 5.22996157]\n",
      " [ 7.5415068 ]\n",
      " [ 9.85305203]\n",
      " [12.16459725]\n",
      " [14.47614248]\n",
      " [16.7876877 ]\n",
      " [19.09923293]\n",
      " [21.41077816]\n",
      " [23.72232338]]\n",
      "\n",
      "y True:\n",
      " [[ 3]\n",
      " [ 5]\n",
      " [ 7]\n",
      " [ 9]\n",
      " [11]\n",
      " [13]\n",
      " [15]\n",
      " [17]\n",
      " [19]\n",
      " [21]]\n"
     ]
    }
   ],
   "source": [
    "# Neural Network Class with 1 hidden layer\n",
    "class LinearRegressionNN:\n",
    "  def __init__(self, input_shape, hidden_units, output_shape):\n",
    "    self.W1 = np.random.random(size=(input_shape, hidden_units))\n",
    "    self.b1 = np.random.random()\n",
    "    self.W2 = np.random.random(size=(hidden_units, output_shape))\n",
    "    self.b2 = np.random.random()\n",
    "\n",
    "    self.a1 = ReLU()\n",
    "    self.a2 = ReLU()\n",
    "\n",
    "    self.loss_fn = MSE()\n",
    "\n",
    "  def set_activation(self, a):\n",
    "    self.a1 = a[0]\n",
    "    self.a2 = a[1]\n",
    "\n",
    "  def set_loss_fn(self, L):\n",
    "    self.loss_fn = L\n",
    "  \n",
    "  def forward(self, x):\n",
    "    a = x @ self.W1 + self.b1\n",
    "    A = self.a1.f(a)\n",
    "    z = A @ self.W2 + self.b2\n",
    "    O = self.a2.f(z)\n",
    "    return a, A, z, O\n",
    "\n",
    "  # x.shape = (m, n), input_shape = n, hidden_units = h, output_shape = o\n",
    "  def backward(self, x, y, a, A, z, O, lr=0.01):\n",
    "    dL_dO = self.loss_fn.fp(y, O)       # shape (m, o)\n",
    "    dO_dz = self.a2.fp(z)               # shape (m, o)\n",
    "    dL_dz = dO_dz * dL_dO               # shape (m, o)\n",
    "\n",
    "    dL_db2 = np.sum(dL_dz)\n",
    "\n",
    "    dz_dW2 = A                          # shape (m, h)\n",
    "    dL_dW2 = dz_dW2.T @ dL_dz           # shape (h, m) @ (m, o) = (h, o)\n",
    "\n",
    "    dz_dA = self.W2                     # shape (h, o)\n",
    "    dL_dA = dL_dz @ dz_dA.T             # shape (m, o) @ (o, h) = (m, h)\n",
    "\n",
    "    dA_da = self.a1.fp(a)               # shape (m, h)\n",
    "    dL_da = dL_dA * dA_da               # shape (m, h)\n",
    "\n",
    "    dL_db1 = np.sum(dL_da)\n",
    "\n",
    "    da_dW1 = x                          # shape (m, n)\n",
    "    dL_dW1 = da_dW1.T @ dL_da           # shape (n, m) @ (m, h) = (n, h)\n",
    "\n",
    "    # Update weights and biases\n",
    "    self.b2 -= lr * dL_db2\n",
    "    self.W2 -= lr * dL_dW2\n",
    "    self.b1 -= lr * dL_db1\n",
    "    self.W1 -= lr * dL_dW1\n",
    "\n",
    "  def train(self, x, y, epochs=1000, lr=0.01):\n",
    "    for epoch in range(epochs):\n",
    "      a, A, z, O = self.forward(x)\n",
    "\n",
    "      if epoch % 100 == 0:\n",
    "        loss = self.loss_fn.f(y, O)\n",
    "        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "\n",
    "      self.backward(x, y, a, A, z, O, lr)\n",
    "\n",
    "    print(\"\\nFinal Params:\")\n",
    "    print(f\"W1: {self.W1} | b1: {self.b1}\")\n",
    "    print(f\"W2: {self.W2} | b2: {self.b2}\")\n",
    "    print(\"\\nPrediction:\\n\", self.forward(x)[3])\n",
    "    print(\"\\ny True:\\n\", y)\n",
    "\n",
    "# Goal y = 2x + 3\n",
    "x = np.array([np.arange(0, 10)]).T\n",
    "y = 2 * x + 3\n",
    "\n",
    "np.random.seed(42)\n",
    "model = LinearRegressionNN(1, 1, 1)\n",
    "model.train(x, y, epochs=3000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataScienceClass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
